{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "xmmhltmxmgfoe2umvkia",
   "authorId": "2029321824938",
   "authorName": "VASOMTSITURI",
   "authorEmail": "vasiko.mtsituri@gmail.com",
   "sessionId": "fe0cfebf-d30a-4657-b717-4dbf323eb21e",
   "lastEditTime": 1766820915087
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col, hash\nfrom snowflake.snowpark.types import DateType, DecimalType, IntegerType, StringType\n\nsession = get_active_session()\n\n# Read stage files\ndf = session.read.json('@\"SNOWFLAKE_CONNECTED_GCS_US\".\"PUBSUB_EVENTS\".\"MY_GCS_STAGE\"/')\ndf = df.select(\n    col('$1')['Meta Data'].alias('METADATA'),\n    col('$1')['Time Series (Daily)'].alias('TIME_SERIES_DAILY'),\n    \n)\n\n# Extract main keys from JSON: \ndf = df.select(\n    col('METADATA')['2. Symbol'].alias('COMPANY'),\n    col('METADATA')['3. Last Refreshed'].alias('LAST_REFRESHED'),\n    col('TIME_SERIES_DAILY')\n)\n\n# Flatten DF with daily stocks price data and cast the types accordingly\ndf = df.flatten(col('TIME_SERIES_DAILY')).select(\n    col('COMPANY').cast(StringType()).alias('COMPANY'),\n    col('LAST_REFRESHED').cast(DateType()).alias('LAST_REFRESHED'),\n    col('KEY').alias('DATE'),\n    col('VALUE')['4. close'].cast(DecimalType(19, 4)).alias('day_closed_value'),\n    col('VALUE')['5. volume'].cast(DecimalType(38, 0)).alias('volume')\n)\n\ndf = df.with_column(\"UNIQUE_ID\",\n                    hash(col(\"COMPANY\"), col(\"DATE\")))\n\n# 9 files with 100 rows each, so it gives us 900 rows\ndf.count()\n\ndf = df.drop_duplicates('UNIQUE_ID')\n\n# API gives rolling last 100 days data, so we should have (number of company * 100) + number of files - 1\ndf.count()\n\ndf.show()\n\ndf.write.mode(\"overwrite\").option(\"single\", True).parquet(\"@PARQUET_FILES_DB.PUBLIC.ALPHA_VANTAGE_MARTS/2025_12_27/stocks.parquet\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}